{
    "circuit_trace_page_title": "Circuit Trace Analysis",
    "circuit_trace_page_desc": "Explore the internal pathways of the OLMo model. This page visualizes how information flows from input tokens through different layers and features to produce the final output, based on a novel Cross-Layer Transcoder method.",
    "how_circuit_tracing_works_header": "How This Works: A Three-Step Process",
    "how_circuit_tracing_works_desc": "Instead of looking at the entire model at once, this technique simplifies the analysis by focusing on 'features'â€”specific, learned patterns of neuron activations. By training small 'transcoder' models, we can identify which features in one layer activate features in the next, allowing us to trace a circuit of information flow.",
    "circuit_tracing_step1_title": "1. Feature Extraction",
    "circuit_tracing_step1_desc": "Small <strong>autoencoder</strong> models (think of them as compression tools that summarize important information) are trained on each layer of the main OLMo model to discover recurring patterns of neuron activations, which we call 'features'.",
    "circuit_tracing_step2_title": "2. Cross-Layer Mapping",
    "circuit_tracing_step2_desc": "Tiny <strong>transcoder</strong> models (which act like translators between layers) are trained to predict the activation of a feature in a later layer based on the activations of features in an earlier layer.",
    "circuit_tracing_step3_title": "3. Graph Construction",
    "circuit_tracing_step3_desc": "By connecting the most predictive feature pairs from the transcoder models, we construct a directed graph that represents the most important pathways of information flow for a given prompt.",
    "enable_ai_explanations_circuit": "Enable AI Explanations",
    "enable_ai_explanations_circuit_help": "Generate detailed explanations for circuit visualizations using Qwen 2.5 VL 72B",
    "about_circuit_tracing_header": "About Circuit Tracing",
    "about_circuit_tracing_body": "Circuit tracing is a technique to understand a model's decision-making by mapping the information flow through its internal components, much like following wires on a circuit board. This is achieved by identifying key 'features' in each layer and then building a graph showing how they influence each other from input to output. This page provides tools like the Interactive Circuit Graph, Feature Explorer, and Subnetwork Explorer to visualize and analyze these computational pathways.",
    "no_results_warning": "Attribution graph results not found.",
    "run_analysis_info": "Please run the analysis script first: `python3 circuit_analysis/attribution_graphs_olmo.py --prompt-index 0 --force-retrain-clt`",
    "config_header": "Configuration",
    "model_label": "Model:",
    "device_label": "Device:",
    "features_per_layer_label": "Features per layer:",
    "training_steps_label": "Training steps:",
    "batch_size_label": "Batch size:",
    "learning_rate_label": "Learning rate:",
    "interactive_analysis_header": "Interactive Analysis",
    "select_prompt_label": "Select a Prompt to Analyze:",
    "select_prompt_help": "This selection controls both the Interactive Circuit Graph and the Feature Explorer below.",
    "graph_stats_header": "Graph Statistics",
    "full_graph_nodes_label": "Full Graph Nodes",
    "full_graph_edges_label": "Full Graph Edges",
    "pruned_graph_nodes_label": "Pruned Graph Nodes",
    "pruned_graph_edges_label": "Pruned Graph Edges",
    "feature_explorer_header": "Feature Explorer",
    "token_analysis_header": "Token Analysis",
    "input_tokens_label": "Input tokens:",
    "feature_explorer_title": "Feature Explorer: {prompt}",
    "select_layer_label": "Select Layer to Explore:",
    "layer_label_format": "Layer {layer_num}",
    "no_feature_viz_warning": "No feature visualizations available for this prompt.",
    "no_features_in_layer_warning": "No features found in {selected_layer}",
    "active_features_label": "**Active Features:**",
    "choose_feature_label": "Choose a feature:",
    "max_activation_label": "Max Activation",
    "mean_activation_label": "Mean Activation",
    "sparsity_label": "Sparsity",
    "interpretation_label": "Interpretation",
    "top_activating_tokens_title": "Top Activating Tokens for {selected_feature}",
    "xaxis_token_label": "Token",
    "yaxis_activation_label": "Activation Strength",
    "generating_feature_explanation_spinner": "Generating AI explanation for feature activation...",
    "feature_explanation_error": "Could not generate feature explanation: {e}",
    "ai_feature_analysis_header": "AI Feature Analysis",
    "node_size_label": "Node Size",
    "edge_threshold_label": "Edge Threshold",
    "tip_scroll_horizontally": "Tip: Use mouse wheel + Shift to scroll horizontally and see all 32 layers",
    "colorbar_title": "Activation",
    "path_highlight_label": "Circuit path",
    "connections_legend": "Connections",
    "embedding_legend": "Embedding",
    "feature_legend": "Feature",
    "layer_nav_header": "Layer Navigation",
    "layer_nav_desc": "This graph shows <strong>{num_layers} layers</strong> with features. Use the range slider below the graph to navigate through all layers, or use <strong>Shift + Mouse Wheel</strong> to scroll horizontally.",
    "generating_circuit_explanation_spinner": "Generating AI explanation for circuit graph...",
    "circuit_explanation_error": "Could not generate circuit explanation: {e}",
    "ai_circuit_analysis_header": "AI Circuit Analysis",
    "layer_stats_header": "Layer Statistics",
    "total_layers_label": "Total Layers with Features",
    "total_features_label": "Total Features",
    "avg_features_per_layer_label": "Avg Features per Layer",
    "features_by_layer_header": "Features by Layer",
    "feature_dist_title": "Feature Distribution Across Layers",
    "feature_count_label": "Feature Count",
    "subnetwork_explorer_title": "Subnetwork Explorer",
    "subnetwork_explorer_desc": "Select a central feature to visualize its local neighborhood, showing both its upstream causes and downstream effects within a specific connection depth.",
    "subnetwork_graph_empty_info": "The main circuit graph has not been generated yet. Please wait for it to load.",
    "no_features_in_graph_warning": "No features are available in the current graph view to build a subnetwork from.",
    "select_layer_label_subnetwork": "1. Select a Layer",
    "no_features_in_layer_subnetwork_warning": "No features to select in {selected_layer}.",
    "select_feature_label_subnetwork": "2. Select a Central Feature",
    "traversal_depth_label": "3. Set Connection Depth",
    "subnetwork_graph_title": "Subnetwork Centered on Feature: {feature}",
    "subnetwork_no_connections_info": "This feature has no connections within the selected depth.",
    "generating_subnetwork_explanation_spinner": "Analyzing subnetwork with AI...",
    "ai_subnetwork_analysis_header": "AI Subnetwork Analysis",
    "subnetwork_analysis_title": "Token Activation Analysis",
    "subnetwork_no_features_info": "No features were found in this subnetwork to analyze.",
    "subnetwork_no_token_info": "No token activation data is available for the features in this subnetwork.",
    "subnetwork_top_tokens_desc": "The following input tokens most strongly activated the features in this subnetwork:",
    "subnetwork_token_interpretation_info": "This shows what parts of the prompt the subnetwork is 'paying attention to.'",
    "what_is_a_feature_header": "Key Concept: What is a 'Feature'?",
    "what_is_a_feature_title": "A feature is a learned, interpretable pattern of neuron activity.",
    "what_is_a_feature_desc": "Think of it as a concept detector. For example, one feature might activate strongly for words related to 'programming,' while another might detect 'questions about history.' These features are the building blocks the model uses to understand input and construct a response. By tracing them, we can map out the model's reasoning process.",
    "faithfulness_check_expander": "Faithfulness Check",
    "running_faithfulness_check_spinner": "Running faithfulness check...",
    "verified_status": "Verified",
    "contradicted_status": "Contradicted",
    "claim_label": "Claim",
    "status_label": "Status",
    "evidence_label": "Evidence",
    "no_verifiable_claims_info": "No verifiable claims were extracted from the explanation.",
    "faithfulness_explanation_circuit_graph_html": "<div style='font-size: 0.9rem; margin-bottom: 1rem;'><strong>How This Works:</strong> The faithfulness checker verifies two types of claims from the AI's explanation:<ul><li><strong>Feature Interpretation Claims:</strong> Checks if a claimed interpretation for a feature in a specific layer (e.g., 'detecting grammatical mood') closely matches an actual feature's interpretation in that layer using fuzzy string matching.</li><li><strong>Layer Role Claims:</strong> Semantically verifies if the AI's summary of a layer section's role (e.g., 'early layers handle syntax') is a plausible generalization of the actual top feature interpretations within that section.</li></ul></div>",
    "faithfulness_explanation_feature_explorer_html": "<div style='font-size: 0.9rem; margin-bottom: 1rem;'><strong>How This Works:</strong> The faithfulness checker verifies two types of claims from the AI's explanation:<ul><li><strong>Top Token Claims:</strong> Checks if a token claimed to be a top activator for a feature is actually present in the list of top activating tokens from the analysis data.</li><li><strong>Feature Role Claims:</strong> Checks if the AI's summarized interpretation of a feature's role closely matches the detailed interpretation from the analysis data using fuzzy string matching.</li></ul></div>",
    "faithfulness_explanation_subnetwork_graph_html": "<div style='font-size: 0.9rem; margin-bottom: 1rem;'><strong>How This Works:</strong> The faithfulness checker verifies three types of claims from the AI's explanation:<ul><li><strong>Causal Claims:</strong> Checks if a claimed causal link (upstream or downstream) is valid by using fuzzy string matching to confirm that the claimed feature's interpretation exists in the actual list of upstream or downstream neighbors.</li><li><strong>Token Influence Claims:</strong> Checks if tokens claimed to be upstream influences are present in the actual list of direct upstream tokens for the central feature.</li><li><strong>Central Feature Role Claims:</strong> Checks if the AI's interpretation of the central feature's role closely matches the interpretation from the analysis data using fuzzy string matching.</li></ul></div>",
    "claim_extraction_prompt_header": "You are an expert claim extraction system. Your task is to read an explanation of a circuit trace visualization and extract verifiable claims into a structured JSON list.",
    "claim_extraction_prompt_instruction": "Each object in the list MUST have: `claim_text`, `claim_type`, and `details`. The `claim_text` should be the full, original sentence from the explanation.",
    "claim_extraction_prompt_rule": "**Extraction Rules:**\n1. **Maintain Original Order.** The claims in the final JSON list must appear in the same order as they do in the source text.\n2. **Ignore legend-like descriptions.** Do not extract claims from sentences that only explain what the visual elements of the graph represent (e.g., 'Each node is a feature', 'Color indicates activation'). Only extract claims that make a specific point about *what the model is doing* for the current prompt (e.g., 'Layer 10 shows high activation for 'syntax' features').\n3. **Keep claims concise.** A single claim should not span an entire paragraph. Break down long paragraphs into multiple, smaller claims, generally one for each main point or a small group of related points.\n4. For each `interpretation_summary` or `role_summary`, extract only the core concept, usually found within single quotes (e.g., from \"notable activity for 'sentence structure'\", extract just \"sentence structure\").\n5. **Crucially, if a single sentence makes multiple claims, you MUST group them into a single claim object.**\n   - For `feature_interpretation_claim`, `details` should be a list of objects, each containing `layer` and `interpretation_summary`.\n   - For `layer_role_claim`, if the claim spans multiple sections (early, middle, late), `details` should be a list of objects, each with `layer_section` and `role_summary`.",
    "claim_extraction_prompt_context_header": "**Context:** {context}",
    "claim_extraction_prompt_types_header": "**Available Claim Types:**",
    "claim_extraction_prompt_analyze_header": "**Explanation to Analyze:**",
    "claim_extraction_prompt_footer": "Respond with ONLY the JSON list of claims. -",
    "circuit_graph_claim_types": "- `feature_interpretation_claim`: A claim about the interpreted role(s) of features in one or more layers. \n  - `details`: A list of objects, e.g., `[{\"layer\": 6, \"interpretation_summary\": \"sentence structure\"}, {\"layer\": 9, \"interpretation_summary\": \"country-related contexts\"}]`\n- `layer_role_claim`: A claim about the general function of one or more layer sections.\n  - `details`: A list of objects, e.g., `[{\"layer_section\": \"early\", \"role_summary\": \"dissect the input\"}, {\"layer_section\": \"middle\", \"role_summary\": \"develop meaning\"}]`",
    "feature_explorer_claim_types": "- `top_token_activation_claim`: A claim that one or more tokens are top activators for the feature.\n  - `details`: { \"tokens\": [\"...\", \"...\"] }\n- `feature_interpretation_claim`: A claim about the feature's role, behavior, significance based on its layer position, or the reasoning for its token activations (e.g., \"Its presence in a late layer indicates...\"). This includes high-level insights. The `details` can be empty if no specific interpretation is mentioned.\n  - `details`: { \"interpretation_summaries\": [\"...\"] }",
    "subnetwork_graph_claim_types": "- `causal_claim`: A claim about upstream (cause) or downstream (effect) relationships. Can involve multiple features.\n  - `details`: { \"source_feature_interpretations\": [\"...\", \"...\"], \"relationship\": \"upstream\" } or { \"target_feature_interpretations\": [\"...\", \"...\"], \"relationship\": \"downstream\" }\n- `feature_interpretation_claim`: A claim about the central feature's function(s).\n  - `details`: { \"interpretation_summaries\": [\"...\"] }\n- `token_influence_claim`: A claim that one or more input tokens are direct upstream influences.\n  - `details`: { \"tokens\": [\"...\"] }\n- `subnetwork_purpose_claim`: A claim about the overall purpose of the subnetwork.\n  - `details`: { \"purpose_summary\": \"...\" }",
    "semantic_verification_prompt_header": "You are an AI fact-checker specializing in transformer model interpretability. Your task is to determine if a \"Claimed Summary\" is a reasonable and faithful semantic summary of the \"Actual Data Points,\" taking into account general knowledge of how transformer layers function.",
    "semantic_verification_prompt_rules_header": "**Crucial Rules:**",
    "semantic_verification_prompt_rule_1": "1.  The summary does not need to use the exact same words as the data points, but it must be semantically consistent.",
    "semantic_verification_prompt_rule_2": "2.  **CRITICAL RULE: General Principles Override Data.** For the **{layer_section}** you are analyzing, the key principle is: *{principle}*. You MUST verify claims that state this widely accepted general principle, even if the specific data points for this prompt do not perfectly align. If a claimed summary matches this principle, you MUST respond with `is_verified: true` and a reasoning that acknowledges it as a correct general principle.",
    "semantic_verification_prompt_rule_3": "3.  **Generalizations are acceptable and expected.** Summaries do not need to list every data point. A high-level, conceptually accurate summary is valid. A claim should be considered verified if it describes a correct aspect of the layer's function, even if it is not a comprehensive summary of all functions. For instance, a claim like 'dissecting the input' is a fair generalization for the early layers' role. **You MUST NOT contradict a claim simply for being 'vague' or 'general' if it is not factually incorrect.**",
    "semantic_verification_principle_early": "**Early layers (approx. 0-10):** Handle syntax, grammar, and basic patterns.",
    "semantic_verification_principle_middle": "**Middle layers (approx. 11-21):** Develop thematic connections, link concepts, and build abstract meaning.",
    "semantic_verification_principle_late": "**Late layers (approx. 22-31):** Synthesize all information to finalize the output.",
    "semantic_verification_prompt_subnetwork_header": "You are an AI fact-checker specializing in transformer model interpretability. Your task is to determine if the 'Claimed Purpose' is a reasonable and faithful semantic summary of the roles of the individual features that make up this computational subnetwork.",
    "semantic_verification_prompt_subnetwork_rules_header": "**Crucial Rules:**",
    "semantic_verification_prompt_subnetwork_rule_1": "1. The purpose does not need to use the exact same words as the data points, but it must be semantically consistent.",
    "semantic_verification_prompt_subnetwork_rule_2": "2. Generalizations are acceptable if they are accurate (e.g., summarizing 'detects punctuation' and 'identifies parts of speech' as 'handling syntax' is a fair generalization).",
    "semantic_verification_prompt_subnetwork_actual_data_header": "**Actual Data Points (Feature interpretations from the subnetwork):**",
    "semantic_verification_prompt_subnetwork_claimed_purpose_header": "**Claimed Purpose:**",
    "semantic_verification_prompt_actual_data_header": "**Actual Data Points (Top feature interpretations from this layer section):**",
    "semantic_verification_prompt_claimed_summary_header": "**Claimed Summary:**",
    "semantic_verification_prompt_task_header": "**Your Task:**",
    "semantic_verification_prompt_task_instruction": "Based on the rules above, is the summary a fair and accurate semantic description of the data? Respond with a JSON object with two keys: `is_verified` (boolean) and `reasoning` (one-sentence explanation). -",
    "semantic_verification_prompt_feature_role_header": "You are an AI fact-checker specializing in transformer model interpretability. Your task is to determine if the 'Claimed Role' is a reasonable and faithful semantic summary of the provided 'Feature Evidence.'",
    "semantic_verification_prompt_feature_role_rules_header": "**Crucial Rules:**",
    "semantic_verification_prompt_feature_role_rule_1": "1. The Claimed Role does not need to use the exact same words as the evidence, but it must be semantically consistent and a plausible interpretation.",
    "semantic_verification_prompt_feature_role_rule_2": "2. Consider the layer position (early/middle/late) as important context. A claim that aligns with the typical function of that layer section is more likely to be correct.",
    "semantic_verification_prompt_feature_role_guidance_early": "Treat claims mentioning foundational grammar, basic sentence structure, or token order as consistent with early-layer behavior even if the exact wording differs from the evidence.",
    "semantic_verification_prompt_feature_role_guidance_middle": "Treat claims about integrating context, linking concepts, or building thematic meaning as consistent with middle-layer behavior even when phrased differently.",
    "semantic_verification_prompt_feature_role_guidance_late": "Treat claims about synthesizing information, finalizing answers, or generating outputs as consistent with late-layer behavior even if the exact words differ from the evidence.",
    "semantic_verification_prompt_feature_role_rule_3": "3. If Upstream or Downstream connections are provided, use them to evaluate claims about the feature acting as a 'bridge', 'hub', or 'integrating' information. The claim should be consistent with the interpretations of the connected features.",
    "semantic_verification_prompt_feature_role_evidence_header": "**Feature Evidence:**",
    "semantic_verification_prompt_feature_role_upstream_header": "- **Upstream Connections (Top Interpretations):** {interpretations}",
    "semantic_verification_prompt_feature_role_downstream_header": "- **Downstream Connections (Top Interpretations):** {interpretations}",
    "semantic_verification_prompt_feature_role_claimed_role_header": "**Claimed Role:**",
    "semantic_verification_prompt_token_reasoning_header": "You are an AI fact-checker specializing in transformer model interpretability. Your task is to determine if the 'Claimed Explanation' for why certain tokens activate a feature is a reasonable and faithful semantic summary of the provided 'Feature Evidence.'",
    "semantic_verification_prompt_token_reasoning_rules_header": "**Crucial Rules:**",
    "semantic_verification_prompt_token_reasoning_rule_1": "1. The explanation does not need to use the exact same words as the evidence, but it must be semantically consistent and a plausible interpretation of the token-feature interaction.",
    "semantic_verification_prompt_token_reasoning_rule_2": "2. Focus on the reasoning provided. The claim is not just that the tokens activate the feature, but *why* they do. Is the explanation logical given the feature's role and layer position?",
    "semantic_verification_prompt_token_reasoning_evidence_header": "**Feature Evidence:**",
    "semantic_verification_prompt_token_reasoning_claimed_explanation_header": "**Claimed Explanation:**",
    "semantic_verification_prompt_causal_reasoning_header": "You are an AI fact-checker specializing in transformer model interpretability. Your task is to determine if the 'Claimed Causal Explanation' is a reasonable and faithful summary of the provided 'Causal Evidence.'",
    "semantic_verification_prompt_causal_reasoning_rules_header": "**Crucial Rules:**",
    "semantic_verification_prompt_causal_reasoning_rule_1": "1. The explanation must be semantically consistent with the roles of the source, central, and target features.",
    "semantic_verification_prompt_causal_reasoning_rule_2": "2. Focus on the reasoning. The claim is not just that a connection exists, but *why* it exists or what its function is. Is the explanation logical?",
    "semantic_verification_prompt_causal_reasoning_evidence_header": "**Causal Evidence:**",
    "semantic_verification_prompt_causal_reasoning_claimed_explanation_header": "**Claimed Causal Explanation:**",
    "explanation_prompt_header": "You are an expert in neural network interpretability and circuit tracing analysis. Analyze this visualization that shows how information flows through the OLMo2 7B language model using Cross-Layer Transcoders.",
    "explanation_prompt_context_header": "## Context",
    "explanation_prompt_instructions_header": "## Instructions",
    "circuit_graph_instruction_header": "Provide a structured, layer-by-layer analysis of the circuit graph. Your response MUST use smaller Markdown headings (`####`). Do not refer to specific feature numbers (e.g., \"feature_411\"); instead, describe their function based on their interpretation.",
    "circuit_graph_instruction_intro": "#### Introduction: What This Graph Shows\nExplain what this specific circuit graph visualizes for the given prompt. Mention that it shows information flow from input tokens through feature activations in different layers.",
    "circuit_graph_instruction_early": "#### Early Layers (0-10): Input Processing\nBased on the top features provided in the context, describe the primary role of these layers. Explain how they deconstruct the input's basic grammar, syntax, or key terms by describing the functions of the active features.",
    "circuit_graph_instruction_middle": "#### Middle Layers (11-21): Developing Meaning\nExplain what these layers do with the initial patterns. Describe how they link concepts, build relationships, or shift the focus of the analysis toward a more abstract understanding.",
    "circuit_graph_instruction_late": "#### Late Layers (22-31): Finalizing the Output\nDescribe how these layers synthesize all the previous information to produce the final result, focusing on how the top features contribute to the model's output.",
    "circuit_graph_instruction_insight": "#### Primary Insight\nConclude with a key takeaway from this analysis. What is the most important or surprising aspect of the model's strategy for this prompt?",
    "circuit_graph_instruction_footer": "Ensure your entire response follows this structure of headings and paragraphs. Do not use bullet points for the main sections.",
    "feature_explorer_instruction_header": "Provide a structured analysis of the feature shown. Your response MUST be a Markdown bulleted list, with each bullet point on a NEW LINE. Use the following structure:",
    "feature_explorer_instruction_role": "- **Feature Role and Layer Context:** Explain the feature's interpretation and what its presence in this specific layer (early/middle/late) implies about its function.",
    "feature_explorer_instruction_activations": "- **Key Token Activations:** Identify the top activating tokens and explain why they are relevant to the feature's role.",
    "feature_explorer_instruction_insight": "- **Overall Insight:** Provide a concluding insight about what this feature's behavior reveals about the model's information processing strategy.",
    "feature_explorer_instruction_footer": "Ensure your output is ONLY this three-bullet list.",
    "subnetwork_graph_instruction_header": "Provide a concise, insightful analysis of this subnetwork. Your response MUST be a Markdown bulleted list, with each bullet point on a NEW LINE. Use the following structure:",
    "subnetwork_graph_instruction_role": "- **Central Feature's Role:** Briefly explain the function of the central feature based on its interpretation and layer position.",
    "subnetwork_graph_instruction_upstream": "- **Upstream Influence:** Describe which earlier features or input tokens (the causes) are most strongly activating this central feature. When mentioning features, refer to their interpretation provided in the context.",
    "subnetwork_graph_instruction_downstream": "- **Downstream Impact:** Describe what later features (the effects) this central feature contributes to most strongly. When mentioning features, refer to their interpretation provided in the context.",
    "subnetwork_graph_instruction_purpose": "- **Subnetwork's Purpose:** Synthesize the above points to hypothesize the overall purpose of this specific computational pathway in processing the prompt.",
    "subnetwork_graph_instruction_footer": "Ensure your output is ONLY this four-bullet list.",
    "context_unspecified_viz": "This is a circuit tracing visualization showing information flow through the model.",
    "instruction_unspecified_viz": "Explain this visualization.",
    "circuit_graph_context_header": "This is a circuit tracing graph for the prompt: \"{prompt}\"",
    "circuit_graph_context_tokens": "Input tokens: {tokens}",
    "circuit_graph_context_summary_header": "#### Key Feature Summary by Layer Section\nHere are the most active features in each section of the model for this prompt:",
    "circuit_graph_context_early_header": "**Early Layers (0-10):**",
    "circuit_graph_context_middle_header": "**Middle Layers (11-21):**",
    "circuit_graph_context_late_header": "**Late Layers (22-31):**",
    "circuit_graph_context_no_features": "No significantly active features found.",
    "circuit_graph_context_feature_line": "- In L{layer}, a feature interpreted as \"{interpretation}\" (Activation: {activation:.2f})",
    "subnetwork_context_header": "This is a subnetwork visualization from a larger circuit trace for the prompt: \"{prompt}\"",
    "subnetwork_context_centered_on": "The subnetwork is centered around:",
    "subnetwork_context_feature": "- **Feature:** {name}",
    "subnetwork_context_layer": "- **Layer:** {layer}",
    "subnetwork_context_interpretation": "- **Interpretation:** \"{interpretation}\"",
    "subnetwork_context_no_interpretation": "No interpretation available.",
    "subnetwork_context_upstream_header": "\nKey Upstream Features (Causes) in this Subgraph:",
    "subnetwork_context_downstream_header": "\nKey Downstream Features (Effects) in this Subgraph:",
    "subnetwork_context_feature_line": "- L{layer} {feature_name}: \"{interpretation}\"",
    "subnetwork_context_depth": "The view shows connections within a depth of **{depth}** hops from the central feature (highlighted in crimson).",
    "subnetwork_context_stats_header": "Subnetwork Statistics:",
    "subnetwork_context_stats_nodes": "- **Nodes:** {nodes}",
    "subnetwork_context_stats_edges": "- **Edges:** {edges}",
    "subnetwork_context_viz_header": "The visualization shows:",
    "subnetwork_context_viz_central": "- The central feature (crimson border) and its neighbors.",
    "subnetwork_context_viz_nodes": "- Upstream nodes (causes) and downstream nodes (effects).",
    "subnetwork_context_viz_lilac": "- Lilac nodes are input token embeddings.",
    "subnetwork_context_viz_other": "- Other nodes are features, colored by activation strength (viridis scale).",
    "subnetwork_context_viz_edges": "- Edge thickness represents connection weights.",
    "feature_explorer_context_header": "This is a feature explorer visualization for the prompt: \"{prompt}\"",
    "feature_explorer_context_model_header": "**Model Context:** The model is OLMo-2-7B, which has 32 layers (indexed 0-31). Layer 0 is the first layer (closest to input embeddings), and Layer 31 is the last layer (closest to the final output). Early layers (e.g., 0-10) handle basic patterns, while late layers (e.g., 22-31) handle more abstract concepts.",
    "feature_explorer_context_analyzing_feature": "We are analyzing **Feature {feature}** in **Layer {layer}**, which is {position} layer in the model.",
    "feature_explorer_context_analyzing_feature_no_pos": "We are analyzing **Feature {feature}** in **Layer {layer}**.",
    "feature_explorer_context_position_early": "an early",
    "feature_explorer_context_position_middle": "a middle",
    "feature_explorer_context_position_late": "a late",
    "feature_explorer_context_tokens": "**Input tokens:** {tokens}",
    "feature_explorer_context_interpretation": "**Feature Interpretation:** \"{interpretation}\"",
    "feature_explorer_context_no_interpretation": "No interpretation available.",
    "feature_explorer_context_footer": "The bar chart shows which input tokens caused the highest activation for this specific feature within its layer. Analyze the relationship between the tokens and the feature's interpretation, keeping the layer's position in mind."
} 